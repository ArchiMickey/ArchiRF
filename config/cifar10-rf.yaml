hparams:
  seed: 42
  batch_size: 128
  grad_accumulate_every: 1
  lr: 2e-4
  # max_grad_norm: torch.inf
  n_steps: 200000
  use_ema: true
  ema_decay: 0.9999

datamodule:
  _partial_: true
  _target_: data.data_utils.ExperimentDataModule
  dataset_name: cifar10
  random_horizonal_flip: true
  root: ./data
  image_size: 32
  num_workers: 8

model:
  _target_: models.rf.RectifiedFlow
  channels: 3
  image_size: ${datamodule.image_size}
  logit_normal_sampling_t: true
  net:
    _target_: modules.networks_unet.UNetModel
    in_channels: 3
    out_channels: 3
    channels: 128
    channel_multipliers: [1, 2, 2, 2]
    attention_levels: [1, 2]
    n_heads: 4
    num_classes: 10
    n_res_block: 2

optimizer:
  _partial_: true
  _target_: bitsandbytes.optim.AdamW8bit
  lr: ${hparams.lr}
  weight_decay: 0.

project_name: flow-experiment

trainer:
  _partial_: true
  _target_: trainer.Trainer
  accelerator:
    _target_: accelerate.Accelerator
    mixed_precision: bf16
    log_with: wandb
  log_dir: ./logs/cifar10/rf
  sample_batch_size: 256
  sampling_steps: 50
  save_and_sample_interval: 20000
  n_per_class: 10
  fid_eval_interval: 100000
  fid_stats_dir: ./results/cifar10
  fid_cfg_scale: 1.5
  num_fid_samples: 50000
  logger_kwargs:
    project_name: flow-experiment
    init_kwargs:
      wandb:
        name: cifar10_rf